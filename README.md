# ğŸ¥ Real-Time Health Monitoring System (Lambda Architecture)

![Status](https://img.shields.io/badge/Status-Prototype-green?style=flat-square)
![Architecture](https://img.shields.io/badge/Architecture-Lambda_Architecture-blue?style=flat-square)
![Tech](https://img.shields.io/badge/Spark-Kafka-orange?style=flat-square)
![Cache](https://img.shields.io/badge/Redis-Stack-red?style=flat-square)

A scalable **IoT Big Data Healthcare System** designed to monitor Diabetes (Glucose) and Heart Rate in real-time. This project implements the **Lambda Architecture** to handle massive data streams with low latency while simultaneously archiving data for historical analysis.

---

## ğŸ§  System Architecture

The system follows the Lambda Architecture pattern, divided into three main layers: **Speed Layer** (Real-time), **Batch Layer** (Historical), and **Serving Layer** (Dashboard).

```mermaid
graph LR
    subgraph "1. Edge Layer"
    A[Sensor Simulator] -->|JSON Stream| B(Kafka Broker)
    end

    subgraph "2. Speed Layer (Real-time)"
    B -->|Stream| C{Spark Structured Streaming}
    C -->|Validate & Process| D[(Redis Cache)]
    end

    subgraph "3. Batch Layer (Historical)"
    B -->|Stream| E[Archiver Script]
    E -->|Write CSV| F[Data Lake / Storage]
    F -->|Read CSV| G{Spark Batch Job}
    G -->|Aggregated Stats| D
    end

    subgraph "4. Serving Layer"
    D -->|Fetch Data| H[Node.js Server]
    H -->|WebSocket| I[Web Dashboard]
    end
```
âœ¨ Key Features
âš¡ Real-Time Visualization: Live streaming graph (Last 60 seconds) for Glucose trends via WebSockets.

ğŸ›¡ï¸ Business Logic Validation: Automatically filters & drops invalid sensor data (e.g., negative heart rates, impossible glucose levels) to ensure data integrity before processing.

ğŸš¨ Smart Alerts: Real-time status classification (NORMAL, WARNING, CRITICAL_HIGH, CRITICAL_LOW).

ğŸ’¾ Data Lake Archiving: Stores raw sensor data into CSV files for audit trails and deep learning.

ğŸ“Š Batch Analytics: On-demand calculation of daily statistics (Average, Min, Max, Total Records) from the Data Lake.

ğŸ§¹ Auto-Initialization: Includes a script to clean the database and prepare the environment instantly.

---

ğŸ› ï¸ Tech Stack
Component,Technology,Description
Ingestion,Apache Kafka,Message Broker for high-throughput data streaming.
Processing,PySpark (Spark 3.x),Engine for both Streaming (Speed) and Batch processing.
Storage,Redis Stack,In-memory database for sub-millisecond dashboard updates.
Data Lake,Local Filesystem,Raw storage in CSV format.
Backend,Node.js + Socket.io,Serves data to the frontend via WebSockets.
Frontend,HTML5 + Chart.js,Responsive dashboard for visualization.
